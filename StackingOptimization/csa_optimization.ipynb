{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('c:/Users/USER/Documents/Data Science Journy/StackingOptimization/driving_data.csv')\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Selected Features: 7                Engine_soacking_time\n",
      "11          Long_Term_Fuel_Trim_Bank1\n",
      "34       Engine_coolant_temperature.1\n",
      "14                 Torque_of_friction\n",
      "51                            Time(s)\n",
      "4                 Intake_air_pressure\n",
      "52                          PathOrder\n",
      "22    Maximum_indicated_engine_torque\n",
      "6          Absolute_throttle_position\n",
      "35     Wheel_velocity_front_left-hand\n",
      "Name: Feature, dtype: object\n",
      "Iteration 1/5\n",
      "Iteration 2/5\n",
      "Iteration 3/5\n",
      "Iteration 4/5\n",
      "Iteration 5/5\n",
      "Best Parameters (Model Selection): [0.565875   0.06645818 0.97067514 0.83356297 0.39552836 0.95539619]\n",
      "Best Fitness (Accuracy): 0.9978147142573339\n",
      "Selected Models: [('rf', RandomForestClassifier(random_state=42)), ('lr', LogisticRegression()), ('svm', SVC(random_state=42)), ('nb', GaussianNB())]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Models: [('rf', RandomForestClassifier(random_state=42)), ('lr', LogisticRegression()), ('svm', SVC(random_state=42)), ('nb', GaussianNB())]\n",
      "\n",
      "Optimized Stacking Model Performance:\n",
      "Accuracy: 0.9983\n",
      "Precision: 0.9983\n",
      "Recall: 0.9983\n",
      "F1 Score: 0.9983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['Class'])  # Features\n",
    "y = df['Class']  # Target\n",
    "\n",
    "# Encode the target variable (Class)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Feature scaling (standardization)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature Selection\n",
    "# Step 1: Random Forest Feature Importance\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_scaled, y_encoded)\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select features above an importance threshold\n",
    "threshold = 0.01\n",
    "important_features = feature_importances[feature_importances['Importance'] > threshold]['Feature']\n",
    "X_important = X[important_features]\n",
    "\n",
    "# Step 2: RFE on Important Features\n",
    "rfe = RFE(estimator=rf_model, n_features_to_select=10)     # 10 Features to selected.\n",
    "X_rfe = rfe.fit_transform(X_important, y_encoded)\n",
    "selected_features = important_features[rfe.support_]\n",
    "\n",
    "print(\"Final Selected Features:\", selected_features)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Select these features \"selected_features\" from X_scaled\n",
    "X_scaled_selected = pd.DataFrame(X_scaled, columns=X.columns)[selected_features]\n",
    "\n",
    "# Split Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_selected, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Available Models\n",
    "available_models = [\n",
    "    (\"rf\", RandomForestClassifier(random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(random_state=42)),\n",
    "    (\"lr\", LogisticRegression()),\n",
    "    (\"svm\", SVC(probability=True, random_state=42)),\n",
    "    (\"knn\", KNeighborsClassifier()),\n",
    "    (\"nb\", GaussianNB())\n",
    "\n",
    "#    (\"rf\", RandomForestClassifier(random_state=42)),\n",
    "#    (\"lr\", LogisticRegression()),\n",
    "#    (\"svc\", SVC(probability=True, random_state=42)),\n",
    "#    (\"knn\", KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "# Define Meta Learner\n",
    "meta_learner = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "def fitness_function(params):\n",
    "    \"\"\"\n",
    "    Example fitness function for CSA. This evaluates the fitness of a single crow's position.\n",
    "    \"\"\"\n",
    "    selected_models = [\n",
    "\n",
    "        (\"rf\", RandomForestClassifier(random_state=42)) if params[0] > 0.5 else None,\n",
    "        (\"xgb\", XGBClassifier(random_state=42)) if params[1] > 0.5 else None,\n",
    "        (\"lr\", LogisticRegression()) if params[2] > 0.5 else None,\n",
    "        (\"svm\", SVC(random_state=42)) if params[3] > 0.5 else None,\n",
    "        (\"knn\", KNeighborsClassifier()) if params[4] > 0.5 else None,\n",
    "        (\"nb\", GaussianNB()) if params[5] > 0.5 else None\n",
    "\n",
    "        #(\"rf\", RandomForestClassifier(random_state=42)) if params[0] > 0.5 else None,\n",
    "        #(\"lr\", LogisticRegression(random_state=42)) if params[1] > 0.5 else None,\n",
    "        #(\"svc\", SVC()) if params[2] > 0.5 else None,\n",
    "        #(\"knn\", KNeighborsClassifier()) if params[3] > 0.5 else None\n",
    "    ]\n",
    "    selected_models = [model for model in selected_models if model is not None]\n",
    "\n",
    "    if not selected_models:  # Penalize if no models are selected\n",
    "        return float('inf')\n",
    "\n",
    "    # Create stacking model\n",
    "    stacking_model = StackingClassifier(estimators=selected_models, final_estimator=LogisticRegression(), cv=5)\n",
    "\n",
    "    # Evaluate with cross-validation\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    try:\n",
    "        X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "        stacking_model.fit(X_train_cv, y_train_cv)\n",
    "        score = stacking_model.score(X_val, y_val)  # Use validation accuracy\n",
    "        return -score  # Minimize negative accuracy\n",
    "\n",
    "        #scores = cross_val_score(stacking_model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "        #return -scores.mean()  # Minimize negative accuracy\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class ParallelCrowSearchAlgorithm:\n",
    "    def __init__(self, n_crows, n_variables, lower_bound, upper_bound, max_iter, fitness_function):\n",
    "        self.n_crows = n_crows\n",
    "        self.n_variables = n_variables\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.max_iter = max_iter\n",
    "        self.fitness_function = fitness_function\n",
    "\n",
    "        # Initialize positions and memory\n",
    "        self.positions = np.random.uniform(low=lower_bound, high=upper_bound, size=(n_crows, n_variables))\n",
    "        self.memory = np.copy(self.positions)\n",
    "        self.memory_fitness = np.full(n_crows, np.inf)\n",
    "\n",
    "    def optimize(self):\n",
    "        for iteration in range(self.max_iter):\n",
    "            print(f\"Iteration {iteration + 1}/{self.max_iter}\")\n",
    "\n",
    "            # Evaluate fitness for all crows in parallel\n",
    "            fitness_values = Parallel(n_jobs=-1)(delayed(self.fitness_function)(params) for params in self.positions)\n",
    "\n",
    "            # Update memory if fitness improves\n",
    "            for i, fitness in enumerate(fitness_values):\n",
    "                if fitness < self.memory_fitness[i]:\n",
    "                    self.memory[i] = self.positions[i]\n",
    "                    self.memory_fitness[i] = fitness\n",
    "\n",
    "            # Update positions\n",
    "            for i in range(self.n_crows):\n",
    "                random_crow = np.random.randint(0, self.n_crows)\n",
    "                r = np.random.uniform(0, 1)\n",
    "                new_position = self.positions[i] + r * (self.memory[random_crow] - self.positions[i])\n",
    "\n",
    "                # Clip to bounds\n",
    "                new_position = np.clip(new_position, self.lower_bound, self.upper_bound)\n",
    "                self.positions[i] = new_position\n",
    "\n",
    "        # Return the best solution\n",
    "        best_index = np.argmin(self.memory_fitness)\n",
    "        return self.memory[best_index], self.memory_fitness[best_index]\n",
    "\n",
    "\n",
    "# Initialize Parallel CSA\n",
    "csa = ParallelCrowSearchAlgorithm(\n",
    "    n_crows=5,               # Number of crows\n",
    "    n_variables=6,            # 4 binary variables for model selection\n",
    "    lower_bound=[0, 0, 0, 0, 0, 0], # Lower bounds\n",
    "    upper_bound=[1, 1, 1, 1, 1, 1], # Upper bounds\n",
    "    max_iter=5,              # Maximum iterations\n",
    "    fitness_function=fitness_function\n",
    ")\n",
    "\n",
    "# Optimize\n",
    "best_params, best_fitness = csa.optimize()\n",
    "print(\"Best Parameters (Model Selection):\", best_params)\n",
    "print(\"Best Fitness (Accuracy):\", -best_fitness)\n",
    "\n",
    "\n",
    "selected_models = [\n",
    "\n",
    "        (\"rf\", RandomForestClassifier(random_state=42)) if best_params[0] > 0.5 else None,\n",
    "        (\"xgb\", XGBClassifier(random_state=42)) if best_params[1] > 0.5 else None,\n",
    "        (\"lr\", LogisticRegression()) if best_params[2] > 0.5 else None,\n",
    "        (\"svm\", SVC(random_state=42)) if best_params[3] > 0.5 else None,\n",
    "        (\"knn\", KNeighborsClassifier()) if best_params[4] > 0.5 else None,\n",
    "        (\"nb\", GaussianNB()) if best_params[5] > 0.5 else None\n",
    "\n",
    "    #(\"RandomForest\" if best_params[0] > 0.5 else None),\n",
    "    #(\"LogisticRegression\" if best_params[1] > 0.5 else None),\n",
    "    #(\"SVC\" if best_params[2] > 0.5 else None),\n",
    "    #(\"KNeighborsClassifier\" if best_params[3] > 0.5 else None)\n",
    "]\n",
    "\n",
    "selected_models = [model for model in selected_models if model is not None]\n",
    "print(\"Selected Models:\", selected_models)\n",
    "\n",
    "\n",
    "# Train Final Model with Selected Models\n",
    "\n",
    "optimized_model = StackingClassifier(\n",
    "    estimators=selected_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5\n",
    ")\n",
    "optimized_model.fit(X_train, y_train)\n",
    "\n",
    "# Test Final Model\n",
    "y_pred = optimized_model.predict(X_test)\n",
    "\n",
    "# Evaluate Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Selected Models:\", selected_models)\n",
    "print(\"\\nOptimized Stacking Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
